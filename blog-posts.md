# Blogs

FloydHub
- [Emil’s Story as a Self-Taught AI Researcher](https://blog.floydhub.com/emils-story-as-a-self-taught-ai-researcher/)

[Lil'Log](https://lilianweng.github.io/lil-log/) - Lilian Weng

[Deep Learning: Our Miraculous Year 1990-1991 - Jürgen Schmidhuber (2019)](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html)

[Berkeley Artificial Intelligence Research - BAIR](https://bair.berkeley.edu/blog/)
- [When Recurrent Models Don't Need to be Recurrent](https://bair.berkeley.edu/blog/2018/08/06/recurrent/)

[lossfunctions](https://lossfunctions.tumblr.com/)

[colah's blog](https://colah.github.io/)
- [Visual Information Theory](https://colah.github.io/posts/2015-09-Visual-Information/)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

[Berkeley Artificial Intelligence Research (BAIR)](https://bair.berkeley.edu/blog/)

Andrej Karpathy - [gitpages blog](http://karpathy.github.io/) - [medium](https://medium.com/@karpathy)
- [A Recipe for Training Neural Networks](http://karpathy.github.io/2019/04/25/recipe/)
- [Software 2.0](https://medium.com/@karpathy/software-2-0-a64152b37c35)
- [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b)
- [A Recipe for Training Neural Networks](https://karpathy.github.io/2019/04/25/recipe/)

[Jürgen Schmidhuber](http://people.idsia.ch/~juergen/)
- [Deep Learning: Our Miraculous Year 1990-1991](http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html)

[Off The Convex Path](http://www.offconvex.org/)
- [When Recurrent Models Don't Need to be Recurrent](http://www.offconvex.org/2018/07/27/approximating-recurrent/) - the trade-offs between recurrent and feed-forward models

[Import AI - Jack Clark](https://jack-clark.net/)

[inconvergent](https://inconvergent.net/)
- [On Generative Algorithms](https://inconvergent.net/generative/)

fastai
- [The problem with metrics is a big problem for AI](https://www.fast.ai/posts/2019-09-24-metrics.html)

[Are You Still Using the Elbow Method?](https://medium.com/data-science/are-you-still-using-the-elbow-method-5d271b3063bd)

The Elbow method is pretty much the worst choice one can do when setting the number of clusters for a dataset.

All the four alternatives that we tested did much better than the Elbow method.

Calinski-Harabasz and BIC performed extremely well, with only one mistake out of 30 datasets.
